<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: Using MobilenetSSD</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('convert_mobilenetssd.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Using MobilenetSSD </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="el" href="convert_mobilenetssd.html#mobilenetssd_conversion_tf">Tensorflow MobilenetSSD model</a> <br />
<a class="el" href="convert_mobilenetssd.html#mobilenetssd_conversion_caffe">Caffe MobilenetSSD model</a> <br />
 </p>
<h1><a class="anchor" id="mobilenetssd_conversion_tf"></a>
Tensorflow MobilenetSSD model</h1>
<p>A specific version of the Tensorflow MobilenetSSD model has been tested: <b>ssd_mobilenet_v1_coco_2017_11_17.tar.gz</b></p>
<p>Download the model. </p><pre class="fragment">wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz</pre><p>After downloading the model extract the contents to a directory.</p>
<pre class="fragment">tar xzvf ssd_mobilenet_v1_coco_2017_11_17.tar.gz</pre><p>Convert the model using the <a class="el" href="tools.html#tools_snpe-tensorflow-to-dlc">snpe-tensorflow-to-dlc</a> converter.</p>
<pre class="fragment">snpe-tensorflow-to-dlc --graph ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb -i Preprocessor/sub 1,300,300,3 --out_node detection_classes --out_node detection_boxes --out_node detection_scores --dlc mobilenet_ssd.dlc --allow_unconsumed_nodes</pre><p>The output layers for the model are: </p><ul>
<li>
Postprocessor/BatchMultiClassNonMaxSuppression </li>
<li>
add_6 </li>
</ul>
<p>The output buffer names are: </p><ul>
<li>
(boxes) Postprocessor/BatchMultiClassNonMaxSuppression_boxes </li>
<li>
(scores) Postprocessor/BatchMultiClassNonMaxSuppression_scores </li>
<li>
(classes) detection_classes:0 </li>
</ul>
<p><b>Training the model</b></p>
<p>The Mobilenet SSD training workflow provided in the <a href="https://github.com/tensorflow/models/tree/master/research/object_detection">Tensorflow website</a> is supported given the following requirements: </p><ol>
<li>
Tensorflow object detection framework from <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config">Tensorflow model github</a> </li>
<li>
Tensorflow version 1.6 </li>
</ol>
<p>After training follow the steps outlined <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md">here</a> in order to create a frozen Tensorflow graph that can be converted by <b>snpe-tensorflow-to-dlc</b>.</p>
<p><b>Running the model in SNPE</b></p>
<p>The following are limitations and suggestions for running DLC model in SNPE: </p><ul>
<li>
Several operations in the model are supported on CPU runtime processor only.<br />
 To run the model using different runtime processor, such as GPU or DSP, CPU fallback mode must be enabled (see SNPEBuilder::setCPUFallbackMode() description in <a href="group__c__plus__plus__apis.html">C++ API</a>).<br />
 If using <a class="el" href="tools.html#tools_snpe-net-run">snpe-net-run</a> tool, use <code>&ndash;enable_cpu_fallback</code> option  </li>
</ul>
<p><br />
</p>
<h1><a class="anchor" id="mobilenetssd_conversion_caffe"></a>
Caffe MobilenetSSD model</h1>
<p>A specific version of the Caffe MobilenetSSD model has been tested: <a href="https://github.com/chuanqi305/MobileNet-SSD">Caffe MobilenetSSD</a>.</p>
<p>There is pre-trained caffe model you can download from <a href="https://github.com/chuanqi305/MobileNet-SSD">Caffe MobilenetSSD</a>.<br />
Download the following two files: </p><pre class="fragment">wget https://github.com/chuanqi305/MobileNet-SSD/blob/master/MobileNetSSD_deploy.prototxt
wget https://github.com/chuanqi305/MobileNet-SSD/blob/master/MobileNetSSD_deploy.caffemodel</pre><p><br />
Convert the model using the <a class="el" href="tools.html#tools_snpe-caffe-to-dlc">snpe-caffe-to-dlc</a> converter.</p>
<pre class="fragment">snpe-caffe-to-dlc --caffe_txt MobileNetSSD_deploy.prototxt --caffe_bin MobileNetSSD_deploy.caffemodel --dlc caffe_mobilenet_ssd.dlc</pre><p>The input and output layers:</p>
<p>Input layer is specified in MobileNetSSD_deploy.prototxt file, via <code>input_shape</code>.<br />
By default, the output layer is the last layer as specified in MobileNetSSD_deploy.prototxt file. In this case that is <code>detection_out</code> (DetectionOutput) layer.</p>
<p><br />
To see info about converted DLC model, use <a class="el" href="tools.html#tools_snpe-dlc-info">snpe-dlc-info</a> tool</p>
<pre class="fragment">snpe-dlc-info -i caffe_mobilenet_ssd.dlc</pre><p>The PriorBox layer is folded by the converter (for model/performance optimization reasons). Consequently, PriorBox layer will not be written into DLC file, hence it will not be listed in DLC info for the model.</p>
<p><b>Training the model</b></p>
<p>To train the model in Caffe, follow instructions at <a href="https://github.com/chuanqi305/MobileNet-SSD">Caffe MobilenetSSD</a>.</p>
<p><b>Running the model in SNPE</b></p>
<p>The following are limitations and suggestions for running DLC model in SNPE: </p><ul>
<li>
Batch dimension &gt; 1 is not supported.  </li>
<li>
DetectionOutput layer is supported on CPU runtime processor only.<br />
 To run the model using different runtime processor, such as GPU or DSP, CPU fallback mode must be enabled (see SNPEBuilder::setCPUFallbackMode() description in <a href="group__c__plus__plus__apis.html">C++ API</a>).<br />
 If using <a class="el" href="tools.html#tools_snpe-net-run">snpe-net-run</a> tool, use <code>&ndash;enable_cpu_fallback</code> option  </li>
<li>
It is recommended to have all DetectionOutput layers in the network listed at the end in the .prototxt file.<br />
 This is to minimize runtime overhead incurred by CPU fallback.<br />
 <a href="https://github.com/chuanqi305/MobileNet-SSD">Caffe MobilenetSSD</a> .prototxt has DetectionOutput layer at the end by default, but if the network has more than one detection output branch, that may not be the case. <br />
 Simply edit .prototxt file, locate and move all DetectionOutput layer definitions to end of the file.  </li>
<li>
Configure DetectionOutput layer reasonably.<br />
 Performance of DetectionOutput layer (i.e. processing time) is function of layer parameters: <code>top_k</code>, <code>keep_top_k</code> and <code>confidence_threshold</code>.<br />
 For example, <code>top_k</code> parameters have practically exponential impact on processing time; e.g. top_k=100 will result in much smaller processing time vs. top_k=1000. Smaller <code>confidence_threshold</code> will result in larger number of boxes to output, and vice versa.  </li>
<li>
Resizing input dimensions at SNPE object creation/build time is not allowed.<br />
 Note that input dimensions are embedded into DLC model during conversion, but in some cases can be overridden via SNPEBuilder::setInputDimensions() (see description in <a href="group__c__plus__plus__apis.html">C++ API</a>) at SNPE object creation/build time. Due to PriorBox layer folding in the model converter, input/network resizing is not possible.  </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
      </p>
    </li>
  </ul>
</div>
</body>
</html>
